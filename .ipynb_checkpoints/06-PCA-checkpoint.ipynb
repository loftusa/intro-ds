{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr/>\n",
    "\n",
    "# Data Mining \n",
    "**Tamás Budavári** - budavari@jhu.edu <br/>\n",
    "\n",
    "- Regularization\n",
    "- Principal Component Analysis\n",
    "- Lagrange multipliers\n",
    "- Explained variance \n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "- A linear combination of known $\\phi_k(\\cdot)$ **basis** functions \n",
    "\n",
    ">$\\displaystyle f(t;\\boldsymbol{\\beta}) = \\sum_{k=1}^K \\beta_k\\, \\phi_k(t) $\n",
    "><br/><br/>\n",
    "> It's a dot product\n",
    ">with $\\boldsymbol{\\beta}=(\\beta_1,\\dots,\\beta_K)^T$\n",
    "\n",
    "- Evaluated at all data points $x=(x_1,x_2,\\dots,x_N)$\n",
    "\n",
    ">$\\displaystyle f(x;\\boldsymbol{\\beta}) = \\boldsymbol{}X\\boldsymbol\\beta$\n",
    "><br/><br/>\n",
    "> where $X_{ik} = \\phi_k(x_i)$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method of Least Squares\n",
    "\n",
    "- At the optimum\n",
    "\n",
    ">$\\displaystyle {\\hat\\beta} = (X^T X)^{-1} X^T {y} $\n",
    "\n",
    "- Hat matrix\n",
    "\n",
    ">$\\hat{y} = X\\boldsymbol{\\hat\\beta} = H {y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a dataset with errors\n",
    "x = 3 * random.rand(50) # uniform between 0 and 3\n",
    "eps = 1 * random.randn(x.size) # normal noise\n",
    "y = 10*cos(x+1) + eps;  plot(x,y,'bx');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly(x,n):\n",
    "    X = np.zeros((x.size,n+1));\n",
    "    for i in range(X.shape[1]):\n",
    "        X[:,i] = x**i\n",
    "    return X\n",
    "\n",
    "# show data in black\n",
    "plot(x,y,'kx'); ylim(-20,20);\n",
    "\n",
    "xx = np.linspace(-1,4,500) # grid on x\n",
    "color = 'yrgbm' * 5 # color sequence\n",
    "for n in range(2):\n",
    "    X = poly(x,n) # design matrix for fitting\n",
    "    bHat = linalg.pinv(X) @ y\n",
    "    yy = poly(xx,n) @ bHat # prediction\n",
    "    plot(xx,yy,'-',c=color[n]);     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "Penalize large coefficients in $\\beta$\n",
    "\n",
    "- **Ridge regression** uses $L_2$\n",
    "\n",
    "> $\\displaystyle \\hat{\\beta} = \\arg\\!\\min_{\\beta} \\, \\lvert y- X\\beta \\,\\rvert^2_2\\ + \\lambda\\,\\lvert\\beta\\rvert^2_2$  \n",
    "><br/>\n",
    "> or even with a constant matrix $\\Gamma$\n",
    "><br/><br/>\n",
    "> $\\displaystyle \\hat{\\beta} = \\arg\\!\\min_{\\beta} \\, \\lvert y- X\\beta \\,\\rvert^2_2 + \\lambda\\,\\lvert\\Gamma\\beta\\rvert^2_2$  \n",
    "\n",
    "- **Lasso regression** uses $L_1$\n",
    "\n",
    "> $\\displaystyle \\hat{\\beta} = \\arg\\!\\min_{\\beta}  \\, \\lvert y- X\\beta \\,\\rvert^2_2 + \\lambda\\,\\lvert\\beta\\rvert_1$ \n",
    "><br/><br/>\n",
    "> $L_1$ yields sparse results\n",
    "\n",
    "Different geometric meanings! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Combinations\n",
    "\n",
    "- Coefficients mix a given set of basis vectors, functions, images, shapes, ...\n",
    "\n",
    "$$ f(x;\\beta) = \\sum_k \\beta_k \\phi_k(x) $$\n",
    "\n",
    "> Fourier series\n",
    "\n",
    "<img src=files/Periodic_identity_function.gif width=400> \n",
    "<!--<img src=https://upload.wikimedia.org/wikipedia/commons/e/e8/Periodic_identity_function.gif width=400> -->\n",
    "\n",
    "> Discrete Cosine Transform (JPEG) \n",
    "\n",
    "<img src=files/DCT_basis_thumb.gif width=200>\n",
    "<!--<img src=http://www.digitude.net/blog/wp-content/uploads/2010/07/DCT_basis_thumb.gif width=200>-->\n",
    "\n",
    "> Spherical Harmonics\n",
    "\n",
    "<img src=https://upload.wikimedia.org/wikipedia/commons/thumb/6/62/Spherical_Harmonics.png/300px-Spherical_Harmonics.png>\n",
    "<!--<img src=https://upload.wikimedia.org/wikipedia/commons/thumb/6/62/Spherical_Harmonics.png/300px-Spherical_Harmonics.png>-->\n",
    "\n",
    "- What is a good basis like?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><font color=\"darkblue\">Principal Component Analysis</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Learning\n",
    "\n",
    ">|                | Supervised     |         Unsupervised     |\n",
    " |:---------------|:--------------:|:------------------------:|\n",
    " | **Discrete**   | Classification | Clustering               |   \n",
    " | **Continuous** | Regression     | Dimensionality Reduction |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=files/800px-GaussianScatterPCA.png width=300 align=left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directions of Maximum Variance\n",
    "\n",
    "- Let $X\\in\\mathbb{R}^N$ be a continuous random variable with $\\mathbb{E}\\left[X\\right]=0$ mean and covariance matrix $C$. What is the direction of maximum variance?\n",
    "\n",
    "> For any vector $a\\in\\mathbb{R}^N$ \n",
    "><br/><br/>\n",
    "> $\\displaystyle \\mathbb{Var}[a^T X] = \\mathbb{E}\\left[(a^T X)(X^T a)\\right] = \\mathbb{E}\\left[a^T(XX^T)\\,a\\right]$\n",
    "><br/><br/>\n",
    "> so\n",
    "><br/><br/>\n",
    "> $\\displaystyle \\mathbb{Var}[a^T X] = a^T\\,\\mathbb{E}\\!\\left[XX^T\\right]\\,a = a^T C\\,a$\n",
    "><br/><br/>\n",
    "> We have to maximize this such that $a^2\\!=\\!1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constrained Optimization\n",
    "\n",
    "- **Lagrange multiplier**: extra term with new parameter $\\lambda$\n",
    "\n",
    "> $\\displaystyle  \\hat{a} = \\arg\\max_{a\\in{}\\mathbb{R}^N} \\left[a^T C\\,a - \\lambda\\,(a^2\\!-\\!1)\\right]$\n",
    "\n",
    "- Partial derivatives vanish at optimum\n",
    "\n",
    "> $\\displaystyle \\frac{\\partial}{\\partial\\lambda} \\rightarrow\\ \\  \\hat{a}^2\\!-\\!1 = 0\\ \\ $  (duh!)\n",
    "><br/><br/>\n",
    "> $\\displaystyle \\frac{\\partial}{\\partial a_k} \\rightarrow\\ \\  $?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With indices\n",
    "\n",
    "\n",
    "> $\\displaystyle \\max_{a\\in{}\\mathbb{R}^N}  \\left[ \\sum_{i,j} a_i C_{ij} a_j - \\lambda\\,\\left(\\sum_i a_i^2 - 1\\right) \\right]$\n",
    "\n",
    "- Partial derivatives $\\partial \\big/ \\partial a_k$ vanish at optimum\n",
    "\n",
    "> $\\displaystyle \\sum_{i,j} \\frac{\\partial a_i}{\\partial a_k} C_{ij} a_j + \\sum_{i,j} a_i C_{ij} \\frac{\\partial a_j}{\\partial a_k} - 2\\lambda\\,\\left(\\sum_i a_i \\frac{\\partial a_i}{\\partial a_k}\\right)  $ \n",
    "> $=\\displaystyle \\sum_{i,j} \\delta_{ik} C_{ij} a_j + \\sum_{i,j} a_i C_{ij} \\delta_{jk} - 2\\lambda\\,\\left(\\sum_i a_i \\delta_{ik}\\right)  $ \n",
    "> $=\\displaystyle \\sum_{j} C_{kj} a_j + \\sum_{i} a_i C_{ik}  - 2\\lambda\\,a_k $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And back again...\n",
    "\n",
    "- With vectors and matrices\n",
    "\n",
    "> $\\displaystyle  C \\hat{a} + C^T\\hat{a} - 2\\lambda \\hat{a} = 0$\n",
    "><br/><br/>\n",
    "> but $C$ is symmetric \n",
    "><br/><br/>\n",
    "> $\\displaystyle  C\\,\\hat{a} = \\lambda\\,\\hat{a} $\n",
    "\n",
    "- Eigenproblem !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "- The value of maximum variance is\n",
    "\n",
    "> $\\displaystyle  \\hat{a}^TC\\,\\hat{a} = \\hat{a}^T \\lambda\\,\\hat{a} = \\lambda\\, \\hat{a}^T\\hat{a} = \\lambda$\n",
    "><br/><br/>\n",
    "> the largest eigenvalue $\\lambda_1$\n",
    "\n",
    "- The direction of maximum variance is the corresponding eigenvector $a_1$\n",
    "\n",
    "> $\\displaystyle  Ca_1 = \\lambda_1 a_1 $\n",
    "\n",
    "- This is the **1st Principal Component** \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd Principal Component\n",
    "\n",
    "- Direction of largest variance uncorrelated to 1st PC\n",
    "\n",
    "> $\\displaystyle  \\hat{a} = \\arg\\max_{a\\in{}\\mathbb{R}^N} \\left[a^T C\\,a - \\lambda\\,(a^2\\!-\\!1) - \\lambda'(a^T C\\,a_1) \\right]$\n",
    "\n",
    "- Partial derivatives vanish at optimum\n",
    "\n",
    "> $\\displaystyle 2C\\,\\hat{a} - 2\\lambda\\,\\hat{a}-\\lambda'Ca_1 = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "- Multiply by $a_1^T\\cdot$\n",
    "\n",
    "> $\\displaystyle 2a_1^TC\\hat{a} - 2a_1^T\\lambda{}\\hat{a}-a_1^T\\lambda'Ca_1 = 0$\n",
    "><br/><br/>\n",
    "> $\\displaystyle 0 - 0 - \\lambda'\\lambda_1 = 0 \\ \\ \\rightarrow\\ \\  \\lambda'=0$\n",
    "\n",
    "- Still just an eigenproblem \n",
    "\n",
    "> $\\displaystyle  C\\,\\hat{a} = \\lambda\\,\\hat{a} $\n",
    "\n",
    "- Solution $\\lambda_2$ and $a_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA \n",
    "\n",
    "- Spectral decomposition or eigenvalue decomposition or eigendecomposition\n",
    "\n",
    "> Let $\\lambda_1\\geq\\lambda_2\\geq\\dots\\geq\\lambda_N\\geq{}0$ be the eigenvalues of $C$ and ${e}_1,\\dots,{e}_N$ the corresponding eigenvectors\n",
    "><br/><br/>\n",
    "> $\\displaystyle  C = \\sum_{k=1}^N\\ \\lambda_k\\left({e}_k\\,{e}_k^T\\right) $\n",
    "><br/><br/>\n",
    "> Consider $\\displaystyle C\\,e_l = \\sum_k \\lambda_k\\,e_k\\left(e_k^T e_l\\right) = \\lambda_l\\,e_l$ for any $l$\n",
    "\n",
    "- Matrix form\n",
    "\n",
    "> With diagonal $\\Lambda$ matrix of the eigenvalues and an $E$ matrix of $[{e}_1, \\dots, {e}_N]$\n",
    "><br/><br/>\n",
    "> $\\displaystyle  C = E\\ \\Lambda\\ E^T$\n",
    "\n",
    "\n",
    "- The eigenvectors of largest eigenvalues capture the most variance\n",
    "\n",
    "> If keeping only $K<N$ eigenvectors, the best approximation is taking the first $K$ PCs\n",
    "><br/><br/>\n",
    "> $\\displaystyle  C \\approx \\sum_{k=1}^K\\ \\lambda_k\\left({e}_k\\,{e}_k^T\\right) =  E_K\\Lambda_KE_K^T$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Coordiante System\n",
    "\n",
    "- The $E$ matrix of eigenvectors is a rotation, $E\\,E^T = I$\n",
    "\n",
    "> $\\displaystyle  Z = E^T\\, X $\n",
    "\n",
    "\n",
    "- A truncated set of eigenvectors $E_K$ defines a projection\n",
    "\n",
    "> $\\displaystyle  Z_K = E_K^T\\, X $\n",
    "><br/><br/>\n",
    "> and\n",
    "><br/><br/>\n",
    "> $\\displaystyle  X_K = E_K Z_K = E_K E_K^T\\, X = P_K\\,X $\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detour: Projections\n",
    "\n",
    "- If the square of a matrix is equal to itself\n",
    "\n",
    "> $\\displaystyle  P^2 = P $\n",
    "\n",
    "- For example, projecting on the ${e}$ unit vector\n",
    "\n",
    "<img src=files/Y7Gx8.png align=right width=250>\n",
    "\n",
    "> Scalar times vector\n",
    "><br/><br/>\n",
    "> $\\displaystyle  r' = {e}\\left({e}^T r\\right) = {e}\\,\\beta_r$\n",
    "><br/><br/>\n",
    "> Or  projection of vector $r$\n",
    "><br/><br/>\n",
    "> $\\displaystyle  r' = \\left({e}\\,{e}^T\\right)r = P\\,r$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Again\n",
    "\n",
    "- The eigenvectors of largest eigenvalues capture the most variance\n",
    "\n",
    "> $\\displaystyle  C \\approx C_K = \\sum_{k=1}^K\\ \\lambda_k\\left({e}_k\\,{e}_k^T\\right) = \\sum_{k=1}^K\\ \\lambda_k\\,P_k$\n",
    "\n",
    "- And the remaining eigenvectors span the subspace with the least variance\n",
    "\n",
    "> $\\displaystyle  C - C_K = %\\sum_{l=K+1}^N\\ \\lambda_l\\left(\\hat{e}_l\\,\\hat{e}_l^T\\right) =\n",
    "\\sum_{l=K+1}^N\\ \\lambda_l\\,P_l$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples\n",
    "\n",
    "- Set of $N$-vectors arranged in matrix $X=\\left[x_1, x_2, \\dots, x_n \\right]$ with average of 0 <br>\n",
    "<font color=\"red\">*This is NOT the random variable we talked about previously but the data matrix!*</font>\n",
    "\n",
    "> Sample covariance matrix is\n",
    "><br/><br/>\n",
    ">$\\displaystyle C = \\frac{1}{n\\!-\\!1}\\ X X^T = \\frac{1}{n\\!-\\!1}\\  \\sum_i x_i x_i^T$\n",
    "\n",
    "- Singular Value Decomposition (SVD)\n",
    "\n",
    ">$\\displaystyle X = U W V^T$\n",
    "><br/><br/>\n",
    "> where $U^TU=I$, $W$ is diagonal, and $V^TV=I$\n",
    "\n",
    "- Hence\n",
    "\n",
    ">$\\displaystyle C = \\frac{1}{n\\!-\\!1}\\  UWV^T\\ VWU^T = \\frac{1}{n\\!-\\!1}\\ U W^2 U^T$\n",
    "><br/><br/>\n",
    "> So, if $C=E\\Lambda E^T$ then $E = U$ and $\\displaystyle \\Lambda = \\frac{1}{n\\!-\\!1}\\  W^2$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Sample from Bivariate Normal \n",
    "\n",
    "- See previous lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "# generate multiple 2-D (column) vectors\n",
    "S = norm.rvs(0,1,(2,20))\n",
    "S[0,:] *= 4  # scale axis 0\n",
    "f = +pi/4    # rotate by 45 degrees\n",
    "R = array([[cos(f), -sin(f)],\n",
    "           [sin(f),  cos(f)]]) \n",
    "X = R.dot(S)\n",
    "X += np.array([[1],[3]]) # shift\n",
    "\n",
    "figure(figsize=(5,5)); xlim(-15,15); ylim(-15,15);\n",
    "plot(X[0,:],X[1,:],'o',alpha=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract sample mean\n",
    "avg = mean(X, axis=1).reshape(X[:,1].size,1)\n",
    "X -= avg\n",
    "# sample covariance matrix\n",
    "C = X.dot(X.T) / (X[0,:].size-1) \n",
    "print (\"Average\\n\", avg)\n",
    "print (\"Covariance\\n\", C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L, E = np.linalg.eig(C)\n",
    "E, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E, L, E_same = np.linalg.svd(C)\n",
    "E, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E.dot(E.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose( E.T, np.linalg.inv(E) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, W, V = np.linalg.svd(X)\n",
    "U, W**2 / (X[0,:].size-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively\n",
    "U, W**2 / (X.shape[1]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ np.allclose( U.dot(U.T), np.eye(U.shape[0]) ), \n",
    "  np.allclose( V.dot(V.T), np.eye(V.shape[0]) )  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "pca = decomposition.PCA(n_components=X.shape[0])\n",
    "pca.fit(X.T) # different convention: row vs col !!!\n",
    "pca.components_.T, pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
