{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr/>\n",
    "\n",
    "# Data Mining\n",
    "**Tamás Budavári** - budavari@jhu.edu <br/>\n",
    "\n",
    "- Dependence and correlations\n",
    "- Sampling from Gaussians \n",
    "- Method of Least Squares\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><font color=\"darkblue\">Bivariate and Multivariate</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependence\n",
    "- Consider random variables $X$, $Y\\in\\mathbb{R}$\n",
    "\n",
    "> We can look at them separately but ...\n",
    ">\n",
    "> Are they \"related\" at all?\n",
    "\n",
    "- Dependent variables\n",
    "\n",
    ">$\\displaystyle P(X, Y) \\neq P(X)\\,P(Y)$ \n",
    "> \n",
    ">More on this later..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance\n",
    "\n",
    "- Definition \n",
    "\n",
    ">$\\displaystyle \\mathbb{Cov}[X,Y]=\\mathbb{E}\\Big[\\big(X-\\mathbb{E}[X]\\big)\\big(Y-\\mathbb{E}[Y]\\big)\\Big]$  \n",
    ">\n",
    "> Other notations: $\\mathrm{C}_{X,Y}$, $\\sigma(X,Y)$, ...\n",
    "\n",
    "- Sample covariance\n",
    "\n",
    ">$\\displaystyle C = \\frac{1}{N\\!-\\!1}\\sum_{i=1}^N (x_i-\\bar{x})(y_i-\\bar{y})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz\n",
    "\n",
    "1) If $X$ and $Y$ are independent, are they also uncorrelated?\n",
    "\n",
    "        [ ] Yes      [ ] No\n",
    "        \n",
    "2) If $X$ and $Y$ are uncorrelated, are they also independent?\n",
    "\n",
    "        [ ] Yes      [ ] No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More examples\n",
    "\n",
    "<img src=\"files/600px-Correlation_examples2.svg.png\" width=500 align=\"left\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Notation\n",
    "\n",
    "- Let $\\boldsymbol{V}$ represent the 2-vector of random scalar variables $X$ and $Y$\n",
    "\n",
    ">$\\boldsymbol{V} = \\begin{pmatrix}X\\\\Y\\end{pmatrix}$\n",
    "\n",
    "- Mean\n",
    "\n",
    ">$\\displaystyle \\mathbb{E}[\\boldsymbol{V}] = \\begin{pmatrix}\\mathbb{E}[X]\\\\\\mathbb{E}[Y]\\end{pmatrix} = \\begin{pmatrix}\\mu_X\\\\\\mu_Y\\end{pmatrix}$\n",
    "\n",
    "- Covariance matrix\n",
    "\n",
    ">$\\displaystyle \\Sigma_{\\boldsymbol{V}} = \\mathbb{E}\\Big[\\big(\\boldsymbol{V}\\!-\\!\\mathbb{E}[\\boldsymbol{V}]\\big)\\big(\\boldsymbol{V}\\!-\\!\\mathbb{E}[\\boldsymbol{V}]\\big)^T\\Big]$ $ = \\left( \\begin{array}{ccc}\n",
    "\\sigma_X^2 & \\mathrm{C}_{X,Y}  \\\\\n",
    "\\mathrm{C}_{Y,X} & \\sigma_Y^2  \\end{array} \\right)$\n",
    ">\n",
    "> Same generalization of variance works in any dimensions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate Normal Distribution\n",
    "- Independent and uncorrelated\n",
    "\n",
    ">$ \\displaystyle {\\cal N}(x,y;\\mu_x,\\mu_y,\\sigma_x,\\sigma_y) = \\frac{1}{2 \\pi \\sigma_x \\sigma_y}\\ \\exp\\left[-\\frac{(x\\!-\\!\\mu_x)^2}{2\\sigma_x^2} -\\frac{(y\\!-\\!\\mu_y)^2}{2\\sigma_y^2} \\right] $\n",
    "\n",
    "- In general, for 2-vector $\\boldsymbol{x}$\n",
    "\n",
    ">$ \\displaystyle{\\cal N}(\\boldsymbol{x};\\boldsymbol{\\mu},\\Sigma) = \\frac{1}{2\\pi \\lvert \\Sigma \\rvert^{\\frac{1}{2}} }\\ \\exp\\left[-\\frac{1}{2} (\\boldsymbol{x}\\!-\\!\\boldsymbol{\\mu})^T \\Sigma^{-1} (\\boldsymbol{x}\\!-\\!\\boldsymbol{\\mu})\\right]$\n",
    ">\n",
    "> where $\\lvert \\Sigma \\rvert$ is the determinant - other notation $\\det\\Sigma$ or $\\det\\!\\left(\\Sigma\\right)$\n",
    "\n",
    "- Uncorrelated if\n",
    "\n",
    ">$ \\displaystyle \\Sigma = \\left( \\begin{array}{ccc}\n",
    "\\sigma_X^2 & 0  \\\\\n",
    "0 & \\sigma_Y^2  \\end{array} \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Normal Distribution\n",
    "\n",
    "- In $k$ dimensions - not bold but $k$-vectors\n",
    "\n",
    "\n",
    ">$ \\displaystyle{\\cal N}(x;\\mu,\\Sigma) = \\frac{1}{\\sqrt{(2\\pi)^{k} \\lvert \\Sigma \\rvert} }\\ \\exp\\left[-\\frac{1}{2} (x\\!-\\!\\mu)^T \\Sigma^{-1} (x\\!-\\!\\mu)\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detour: Another way to Sample from Gaussians\n",
    "- Uncorrelated ${\\cal N}(0,I)$: Box-Muller transform \n",
    "\n",
    "> Using 2 uniform randoms between 0 and 1\n",
    ">\n",
    ">$ Z_1 = \\sqrt{-2\\ln U_1}\\,\\cos\\big(2\\pi U_2\\big)$ \n",
    ">\n",
    ">$ Z_2 = \\sqrt{-2\\ln U_1}\\,\\sin\\big(2\\pi U_2\\big)$ \n",
    "\n",
    "- Tranform: scale, rotate, shift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline \n",
    "from scipy.stats import norm as gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate many 2D (column) vectors\n",
    "X = gaussian.rvs(0,1,(2,1000))\n",
    "X[0,:] *= 3  # scale axis 0\n",
    "f = +pi/4    # rotate by f\n",
    "R = array([[cos(f),-sin(f)],\n",
    "           [sin(f), cos(f)]]) \n",
    "V = R.dot(X)\n",
    "V += np.array([[2],\n",
    "               [5]]) # shift with a vector\n",
    "# plot on square figure\n",
    "figure(figsize=(4,4)); a=15; xlim(-a,a); ylim(-a,a)\n",
    "plot(V[0,:],V[1,:], '.', alpha=0.1)\n",
    "\n",
    "# sample covariance matrix\n",
    "averages = mean(V, axis=1)\n",
    "print (averages.shape)\n",
    "averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#avg = averages.reshape(averages.size,1)\n",
    "avg = averages[:,np.newaxis] \n",
    "print (\"Average: \")\n",
    "print (avg)\n",
    "print (\"Cov:\")\n",
    "print (np.dot(V-avg, (V-avg).T) / (V[0,:].size-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><font color=\"darkblue\">Method of Least Squares</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Idea\n",
    "\n",
    "- Fit a model to training set $\\big\\{ (x_i, y_i) \\big\\}$\n",
    "\n",
    "> Parameterized function $f(x;\\theta)$, where $\\theta$ can represent multiple parameters\n",
    "\n",
    "- Minimize the mean or sum of square errors or residuals (SSE, SSR, MSE, MSR?)\n",
    "\n",
    "> Residual   \n",
    ">$r_i(\\theta) = y_i - f(x_i;\\theta)$\n",
    ">\n",
    "> Estimation  \n",
    ">$\\displaystyle \\hat{\\theta} = \\arg\\min_{\\theta} \\sum_i \\big[y_i-f(x_i;\\theta)\\big]^2$\n",
    " \n",
    "- Optimization is simple for certain models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Simplest Case\n",
    "- Fitting a constant? Model with $f(x;\\mu)=\\mu$\n",
    "\n",
    ">$\\displaystyle C(\\mu) = \\sum_{i=1}^N \\big(y_i\\!-\\!\\mu\\big)^2$\n",
    "\n",
    "- Derivative $C'= dC \\big/ d\\mu$ vanishes at solution $\\hat{\\mu}$\n",
    "\n",
    ">$\\displaystyle C'(\\hat{\\mu}) = 0$\n",
    ">\n",
    ">$\\displaystyle 2\\sum_{i=1}^N \\big(y_i\\!-\\!\\hat{\\mu}\\big)(-1)=0$\n",
    ">\n",
    ">$\\displaystyle \\sum_{i=1}^N y_i - N \\hat{\\mu} = 0 $\n",
    ">\n",
    ">$\\displaystyle \\hat{\\mu} = \\frac{1}{N}\\sum_{i=1}^N y_i \\ \\ \\ \\ \\ $  -  average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heteroscedasticity\n",
    "- Same model with $f(x;\\mu)=\\mu$\n",
    "\n",
    ">$\\displaystyle C(\\mu) = \\sum_{i=1}^N \\frac{\\big(y_i\\!-\\!\\mu\\big)^2}{\\sigma_i^2} $\n",
    ">\n",
    "> with  $\\ w_i=1 \\big/ \\sigma_i^2$\n",
    ">\n",
    ">$\\displaystyle  C(\\mu) = \\sum_{i=1}^N w_i\\big(y_i\\!-\\!\\mu\\big)^2 $\n",
    "\n",
    "- Derivative $C'= dC \\big/ d\\mu$ vanishes at $\\hat{\\mu}$\n",
    "\n",
    ">$\\displaystyle C'(\\hat{\\mu}) = 0$\n",
    ">\n",
    ">$\\displaystyle 2\\sum_{i} w_i \\big(y_i\\!-\\!\\hat{\\mu}\\big)(-1)=0$\n",
    ">\n",
    ">$\\displaystyle \\sum_{i} w_i y_i - \\hat{\\mu}\\sum_{i} w_i  = 0 $\n",
    ">\n",
    ">$\\displaystyle \\hat{\\mu} = \\frac{\\sum w_i y_i}{\\sum w_i} \\ \\ \\ \\ \\ \\ $ - weighted average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Fitting\n",
    "- A linear model with $\\boldsymbol{\\theta}=(a,b)^T$ parametrization $f(x;\\boldsymbol{\\theta}) = a + b\\,x$\n",
    "  \n",
    ">$\\displaystyle \\hat{\\boldsymbol\\theta} = \\arg\\min \\sum_i \\big[y_i-(a + b\\,x_i)\\big]^2$\n",
    "\n",
    "- Derivatives w.r.t. $a$ and $b$ should vanish\n",
    "\n",
    "> We have 2 variables and 2 equations\n",
    "> <br/><br/>\n",
    "> Quadratic becomes linear $\\rightarrow$ analytic solution!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unhomework\n",
    "\n",
    "1. Derive the best fit parameters of $(a,b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "- A linear combination of known $\\phi_k(\\cdot)$ functions (basis functions)\n",
    "\n",
    ">$\\displaystyle f(x;\\boldsymbol{\\beta}) = \\sum_{k=1}^K \\beta_k\\, \\phi_k(x) $\n",
    "> <br/><br/>\n",
    "> It's a dot product\n",
    "> <br/><br/>\n",
    ">$\\displaystyle f(x;\\boldsymbol{\\beta}) = \\boldsymbol\\beta^T \\boldsymbol\\phi(x)$ \n",
    "> <br/><br/>\n",
    ">with $\\boldsymbol{\\beta}=(\\beta_1,\\dots,\\beta_K)^T$\n",
    "\n",
    "\n",
    "- Linear in $\\boldsymbol{\\beta}$, cost function is quadratic\n",
    "\n",
    ">$\\displaystyle C = \\sum_{i=1}^N \\left\\{ y_i - \\sum_{k=1}^K \\beta_k\\, \\phi_k(x_i) \\right\\}^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Introducing matrix $X$ with components\n",
    "\n",
    ">$\\displaystyle X_{ik} = \\phi_k(x_i) $\n",
    "\n",
    "- Linear in $\\boldsymbol{\\beta}$, cost function is quadratic\n",
    "\n",
    ">$\\displaystyle C = \\sum_{i=1}^N \\left\\{ y_i - \\sum_{k=1}^K X_{ik}\\beta_k\\right\\}^2$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimization\n",
    "\n",
    "- Partial derivatives\n",
    "  \n",
    "\n",
    ">$\\displaystyle \\frac{\\partial{}C}{\\partial{}\\beta_l} = 2\\sum_i \\left\\{ y_i - \\sum_{k=1}^K X_{ik}\\beta_k \\right\\} \n",
    "\\left[ -\\frac{\\partial f(x_i;\\boldsymbol{\\beta})}{\\partial \\beta_l} \\right]$\n",
    "> <br/><br/>\n",
    "> and\n",
    "> <br/><br/>\n",
    ">$\\displaystyle \\frac{\\partial f(x_i;\\boldsymbol{\\beta})}{\\partial \\beta_l} =\\sum_k \\frac{\\partial{}\\beta_k}{\\partial{}\\beta_l}\\,\\phi_k(x_i) = \\phi_l(x_i) = X_{il}$\n",
    "> <br/><br/>\n",
    "> **Note:** $\\partial{}\\beta_k \\big/ \\partial{}\\beta_l=\\delta_{kl}$ Kronecker delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detour: The Kronecker Delta\n",
    "\n",
    "- Definition\n",
    "\n",
    ">$ \\delta_{kl} = \\left\\{ \\begin{array}{ll}\n",
    "         1 & \\mbox{if $k=l$}\\\\\n",
    "         0 & \\mbox{if $k\\neq{}l$}\\end{array} \\right.  $\n",
    "         \n",
    "- Useful to remember\n",
    "\n",
    ">$ \\displaystyle \\sum_l \\delta_{kl}\\,a_l = a_k$\n",
    "> <br/><br/>\n",
    "> Cf. identity matrix:\n",
    ">$ I\\, \\boldsymbol{a} = \\boldsymbol{a}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "- At the optimum we have\n",
    "\n",
    "\n",
    ">$\\displaystyle \\sum_i \\left\\{ y_i - \\sum_{k} \\hat\\beta_k\\, \\phi_k(x_i) \\right\\}\\, \\phi_l(x_i)  = 0$\n",
    "> <br/><br/>\n",
    ">$\\displaystyle \\sum_i \\left\\{ y_i - \\sum_{k} X_{ik}\\hat\\beta_k\\ \\right\\}\\, X_{il}  = 0$\n",
    "> <br/><br/>\n",
    ">$\\displaystyle \\sum_i X_{il} y_i  - \\sum_i \\sum_k  X_{il} X_{ik} \\hat{\\beta}_k = 0$\n",
    "> <br/><br/>\n",
    ">$\\displaystyle \\sum_i  X_{il} y_i = \\sum_k \\left(\\sum_i X_{il} X_{ik}\\right) \\hat{\\beta}_k$\n",
    "\n",
    "- I.e.,\n",
    "\n",
    ">$\\displaystyle X^T y = X^T X \\hat{\\beta} $\n",
    "> <br/><br/>\n",
    ">$\\displaystyle \\hat\\beta = (X^T X)^{-1} X^T y = X^+ y$\n",
    "\n",
    "- See **Moore-Penrose pseudoinverse**, **generalized inverse**\n",
    "\n",
    "- See also **Singular Value Decomposition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hat matrix\n",
    "\n",
    "- Looking at the definition of $X$ we see that the model at $\\hat{\\beta}$ predicts $\\hat{y}_i$ values\n",
    "\n",
    ">$\\displaystyle \\hat{y} = X\\,\\hat\\beta = X\\,(X^T X)^{-1} X^T y $\n",
    "> <br/><br/>\n",
    "> which is\n",
    "> <br/><br/>\n",
    ">$\\displaystyle \\hat{y}  = H\\,y\\ \\ \\ $\n",
    "> with \n",
    ">$\\ \\ \\displaystyle {H} = X\\,(X^T X)^{-1} X^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sample with error\n",
    "x = 3 * random.rand(50) # between 0 and 3\n",
    "e = 1 * random.randn(x.size) # noise\n",
    "#y = (0.1*x**3 + 0.5*x**2 + 2*x + 1) + e;  plot(x,y,'bo');\n",
    "y = 10*cos(x+1) + e;  plot(x,y,'bx');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear model f(x) = b0 + b1 x\n",
    "X = np.ones((x.size,2));\n",
    "X[:,1] = x\n",
    "\n",
    "Xpinv = dot(inv(dot(X.T,X)),X.T)\n",
    "bHat = dot(Xpinv,y)\n",
    "yHat = dot(X,bHat)\n",
    "\n",
    "plot(x,y,'bx'); plot(x,yHat,'ro'); bHat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same using methods\n",
    "Xpinv = inv(X.T.dot(X)).dot(X.T)\n",
    "bHat = Xpinv.dot(y)\n",
    "yHat = X.dot(bHat)\n",
    "\n",
    "plot(x,y,'bx'); plot(x,yHat,'ro'); bHat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same again with pinv() and the Hat matrix\n",
    "H = X.dot(linalg.pinv(X)) \n",
    "yHat = H.dot(y)\n",
    "\n",
    "plot(x,y,'bx'); plot(x,yHat,'ro');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear model f(x) = b0 + b1 x + b2 * x^2\n",
    "X = np.ones((x.size,3));\n",
    "X[:,1] = x # partials wrt. b1\n",
    "X[:,2] = x*x # wrt. b2\n",
    "\n",
    "# sames as before\n",
    "bHatQ = linalg.pinv(X).dot(y)\n",
    "yHatQ = X.dot(bHatQ)\n",
    "\n",
    "# or like this\n",
    "H = dot(X,linalg.pinv(X))\n",
    "yHatQ = dot(H,y)\n",
    "\n",
    "plot(x,y,'bx'); plot(x,yHatQ,'ro');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unhomework\n",
    "\n",
    "1. Fit a 3rd order polynomial to the same data\n",
    "\n",
    "1. Fit $f(x;\\beta_0,\\beta_1) = \\beta_0\\sin(x) + \\beta_1\\cos(x)$\n",
    "\n",
    "1. Evaluate the best fits on a grid of 1000 equally-spaced points in $[-1,4]$\n",
    "\n",
    "1. Plot them in one figure"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
