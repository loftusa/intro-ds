{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr/>\n",
    "\n",
    "# Introduction to Data Science\n",
    "**Tamás Budavári** - budavari@jhu.edu <br/>\n",
    "\n",
    "- Classification problems\n",
    "- Nearest Neighbors (NN, $k$-NN)\n",
    "- Naive Bayes Classifier\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><font color=\"darkblue\">Classification</font></h1>\n",
    "\n",
    "- Based on a **training set** of labeled points, assign class labels to unknown vectors in the **query set**.  \n",
    "\n",
    "> **Training set**\n",
    "><br><br>\n",
    ">$T = \\big\\{ (x_i, C_i) \\big\\}$ where $x_i\\in \\mathbb{R}^d$ and $C_i$ is the known class membership \n",
    "\n",
    "> **Query set**\n",
    "><br><br>\n",
    ">$Q = \\big\\{ x_i \\big\\}$ where $x_i\\in \\mathbb{R}^d$ \n",
    "\n",
    "> For example,\n",
    "> blood tests ($x$) and sick/healthy ($C$) - we want to predict if a new patient is sick based on the available measurements\n",
    "\n",
    "- Similar to regression but with discrete categories to classify into..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification Methods\n",
    "\n",
    "- $k$-NN\n",
    "- Naive Bayes\n",
    "- Quadratic Discriminant Analysis\n",
    "- Logistic regression\n",
    "- Decisions trees\n",
    "- Random forests\n",
    "- Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Iris Dataset\n",
    "\n",
    "We'll use this data set available in [scikit-learn](http://scikit-learn.org/stable/index.html), see [this](http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html) page for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "[k for k in iris]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (iris['target_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.feature_names, iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (type(iris.data))\n",
    "print (iris.data.shape)\n",
    "\n",
    "# have a peek\n",
    "print (iris.data[:5])\n",
    "print (iris.target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot two features\n",
    "i,j = 0,1\n",
    "\n",
    "sizes = np.array([70,50,20])\n",
    "plt.scatter(iris.data[:,i], iris.data[:,j], edgecolor='none', \n",
    "            c=iris.target, s=sizes[iris.target], cmap=cm.brg, alpha=0.5); \n",
    "colorbar();\n",
    "xlabel(iris.feature_names[i]);\n",
    "ylabel(iris.feature_names[j]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see also http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# whiten the data and use top 3 components\n",
    "pca = PCA(n_components=3,whiten=True)\n",
    "b = pca.fit_transform(iris.data)\n",
    "\n",
    "# 3D plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(b[:,0], b[:,1], b[:,2], c=iris.target)\n",
    "ax.set_xlabel(\"1st\");\n",
    "ax.set_ylabel(\"2nd\");\n",
    "ax.set_zlabel(\"3rd\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Nearest Neighbor\n",
    "\n",
    "\n",
    "- Assign label or value of nearest neighbor (NN) in the training set\n",
    "\n",
    "> Simple but powerful\n",
    "> <br>\n",
    "> <img src=\"files/KnnClassification.svg\" width=200 align=center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive and very slow nearest neighbor search\n",
    "# for illustration purposes only...\n",
    "import datetime as dt\n",
    "\n",
    "X = iris.data[:,:]\n",
    "y = iris.target\n",
    "\n",
    "print ('Unique classes:', np.unique(iris.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = -1 * np.ones(y.size)\n",
    "\n",
    "start = dt.datetime.now()\n",
    "\n",
    "# loop on query set\n",
    "for i in arange(y.size): \n",
    "    \n",
    "    d2min = 1e99  # something large\n",
    "    \n",
    "    # loop on training set\n",
    "    for j in arange(y.size):\n",
    "        if i != j:               # leave one out\n",
    "            d = X[i,:] - X[j,:]  # diff vector\n",
    "            d2 = d.dot(d)        # its length squared\n",
    "            if d2 < d2min:       # check if closer\n",
    "                d2min = d2       # save it \n",
    "                y_pred[i] = y[j]\n",
    "\n",
    "print ('Elapsed time', dt.datetime.now() - start)\n",
    "\n",
    "print (\"Number of mislabeled points out of a total %d points: %d\" \n",
    "       % (iris.target.size, (y!=y_pred).sum()))\n",
    "\n",
    "# write a faster version of this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $k$ Nearest Neighbors\n",
    "\n",
    "- Assign label or value based $k$ nearest neighbors ($k$-NN) in the training set\n",
    "\n",
    "> For example, the most frequent \"vote\" <br/> possibly with weighting\n",
    "> <br>\n",
    "> <img src=\"files/KnnClassification.svg\" width=200>\n",
    "\n",
    "- Using $k$ instead of a distance cutoff helps with large density contrasts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "X = iris.data[:,:2] # using only 2 features for each\n",
    "y = iris.target\n",
    "\n",
    "start = dt.datetime.now()\n",
    "clf = neighbors.KNeighborsClassifier(4)\n",
    "y_pred = clf.fit(X,y).predict(X)\n",
    "\n",
    "print (\"Elapsed time\", dt.datetime.now()-start)\n",
    "print(\"Number of mislabeled points out of a total %d points: %d\"\n",
    "      % (iris.target.size, (iris.target!=y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where did we cheat?\n",
    "\n",
    "- Can you spot the problem with the code above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on a grid\n",
    "\n",
    "- Create a mesh of points with resolution $h$\n",
    "- Plot classification results for each grid point\n",
    "- Visualize results\n",
    "- Do it for different $k$NN weighting schemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a grid of points \n",
    "gx,gy = np.meshgrid([1,2,3], [10,20])\n",
    "print (gx)\n",
    "print (gy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gx.ravel(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.c_[gx.ravel(), gy.ravel()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create color maps\n",
    "from matplotlib.colors import ListedColormap\n",
    "cmap_light = ListedColormap(['#FFBBBB', '#BBFFBB', '#BBBBFF'])\n",
    "cmap_bold = ListedColormap(['#CC0000', '#00AA00', '#0000CC'])\n",
    "\n",
    "n_neighbors = 30\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:,:2]  # we only take the first two features\n",
    "y = iris.target\n",
    "\n",
    "h = 0.01 # step size in the mesh\n",
    "\n",
    "# Points in a mesh of [x_min, m_max] x [y_min, y_max]\n",
    "x_min, x_max = X[:,0].min()-1, X[:,0].max()+1\n",
    "y_min, y_max = X[:,1].min()-1, X[:,1].max()+1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "for weights in ['uniform', 'distance']:\n",
    "    \n",
    "    # we create an instance of Neighbours Classifier and fit the data.\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    Z = clf.predict(grid)\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    if False:\n",
    "        plt.scatter(xx, yy, c=Z, cmap=cmap_light, edgecolor='none')\n",
    "    else:\n",
    "        plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:,0], X[:,1], c=y, cmap=cmap_bold)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\"3-Class classification (k = %i, weights = '%s')\"\n",
    "              % (n_neighbors, weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise / Unhomework \n",
    "\n",
    "- Which two features work best to predict the classes of the iris dataset?\n",
    "- How much better/worse than using all features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other examples\n",
    "\n",
    "More [exercises](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#examples-using-sklearn-neighbors-kneighborsclassifier) are available at http://scikit-learn.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Meaningful Distance?\n",
    "\n",
    "- Need a distance function\n",
    "\n",
    "> E.g., use Euclidean distance in $\\mathbb{R}^d$\n",
    "\n",
    "- Problem with different features and units\n",
    "\n",
    "> In practice, **centering** and **scaling** often helps <br/>\n",
    "> Arguably, black art...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Curse of Dimensionality\n",
    "\n",
    "- Everybody is lonely in high dimensions\n",
    "\n",
    "> Surface / Volume ratio grows <br/>as function of $d$, the dimension\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
