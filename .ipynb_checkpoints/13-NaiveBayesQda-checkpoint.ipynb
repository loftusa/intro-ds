{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr/>\n",
    "\n",
    "# Introduction to Data Science\n",
    "**Tamás Budavári** - budavari@jhu.edu <br/>\n",
    "\n",
    "- Classification problems\n",
    "- Naive Bayes Classifier\n",
    "- Quadratic Discriminant Analysis\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><font color=\"darkblue\">Classification</font></h1>\n",
    "\n",
    "- Based on a **training set** of labeled points, assign class labels to unknown vectors in the **query set**.  \n",
    "\n",
    "> **Training set**\n",
    "><br><br>\n",
    ">$T = \\big\\{ (\\boldsymbol{x}_i, C_i) \\big\\}_{i=1}^N$ where $x_i\\in \\mathbb{R}^d$ and $C_i$ is the known class membership \n",
    "\n",
    "> **Query set**\n",
    "><br><br>\n",
    ">$Q = \\big\\{ \\boldsymbol{x}_j \\big\\}_{j=1}^M$ where $x_j\\in \\mathbb{R}^d$ \n",
    "\n",
    "> For example,\n",
    "> blood test results ($\\boldsymbol{x}$) and sick/healty ($C$) - we want to predict if a new patient is sick based on the available measurements\n",
    "\n",
    "- Similar to regression but with discrete categories to classify into..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification Methods\n",
    "\n",
    "- $k$-NN\n",
    "- Naive Bayes\n",
    "- Linear Discriminant Analysis\n",
    "- Logistic regression\n",
    "- Decisions trees\n",
    "- Random forests\n",
    "- Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Iris Dataset\n",
    "\n",
    "We'll use this data set available in [scikit-learn](http://scikit-learn.org/stable/index.html), see [this](http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html) page for details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other examples\n",
    "\n",
    "More [exercises](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#examples-using-sklearn-neighbors-kneighborsclassifier) are available at http://scikit-learn.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k$ Nearest Neighbors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique known classes in training set\n",
    "classes = np.unique(iris.target)\n",
    "print ('There are %d classes:' % len(classes), classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "# create color maps\n",
    "from matplotlib.colors import ListedColormap\n",
    "cmap_light = ListedColormap(['#FFBBBB', '#BBFFBB', '#BBBBFF'])\n",
    "cmap_bold = ListedColormap(['#CC0000', '#00AA00', '#0000CC'])\n",
    "\n",
    "n_neighbors = 1\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:,:2]  # we only take the first two features\n",
    "y = iris.target\n",
    "\n",
    "h = 0.05  # step size in the mesh\n",
    "\n",
    "x_min, x_max = X[:,0].min()-1, X[:,0].max()+1\n",
    "y_min, y_max = X[:,1].min()-1, X[:,1].max()+1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "for weights in ['uniform', 'distance']:\n",
    "    \n",
    "    # we create an instance of Neighbours Classifier and fit the data.\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n",
    "    clf.fit(X, y)\n",
    "    \n",
    "    ZZ = clf.predict(grid)\n",
    "    ZZ = ZZ.reshape(xx.shape) # 2-D grid layout\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each grid point\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.pcolormesh(xx, yy, ZZ, cmap=cmap_light)\n",
    "    \n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:,0], X[:,1], c=y, cmap=cmap_bold)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\"3-Class classification (k = %i, weights = '%s')\"\n",
    "              % (n_neighbors, weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meaningful Distance?\n",
    "\n",
    "- Need a distance function\n",
    "\n",
    "> E.g., use Euclidean distance in $\\mathbb{R}^d$\n",
    "\n",
    "- Problem with different features and units\n",
    "\n",
    "> In practice **centering** and **scaling** often helps <br/>\n",
    "> Arguably, black art...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes Classifier\n",
    "\n",
    "- In general, we can use Bayes' rule (and law of total probability) to infer discrete classes $C_k$ for a given $\\boldsymbol{x}$ set of features\n",
    "\n",
    ">$\\displaystyle P(C_k \\lvert\\,\\boldsymbol{x}) = \\frac{\\pi(C_k)\\,{\\cal{}L}_{\\!\\boldsymbol{x}}(C_k)}{Z} $ \n",
    "\n",
    "\n",
    "- Naively assuming the features are independent \n",
    "\n",
    ">$\\displaystyle {\\cal{}L}_{\\!\\boldsymbol{x}}(C_k) = \\prod_{\\alpha}^d p(x_{\\alpha} \\lvert C_k)$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes: Learning\n",
    "\n",
    "- Say for Gaussian likelihoods, we simply estimate the sample mean and variance of all features for each class $k$\n",
    "\n",
    ">$\\displaystyle p(x_{\\alpha} \\lvert C_k) = G(x_{\\alpha};\\mu_{k,\\alpha}, \\sigma^2_{k,\\alpha})$\n",
    "\n",
    "- We have to also pick some prior for the classes\n",
    "\n",
    "> Using uniform or based on frequency of points in the training set?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes: Estimation\n",
    "\n",
    "- Look for maximum of the posterior\n",
    "\n",
    "\n",
    ">$\\displaystyle \\hat{k} =  \\mathrm{arg}\\max_k \\left[ \\pi_k \\prod_{\\alpha}^d G(x_{\\alpha};\\mu_{k,\\alpha}, \\sigma^2_{k,\\alpha})\\right]$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate feature means and variances for each class\n",
    "param = dict()  # we save them in this dictionary\n",
    "data = iris.data.copy()\n",
    "for k in classes:\n",
    "    members = (iris.target == k) # boolean array\n",
    "    num = members.sum()    # True:1, False:0\n",
    "    prior = num / iris.target.size\n",
    "    X = data[members,:] # slice out members\n",
    "    mu = X.mean(axis=0)      # calc mean\n",
    "    X -= mu\n",
    "    var = (X*X).sum(axis=0) / (X.shape[0]-1)\n",
    "    param[k] = (num, prior, mu, var) # save results\n",
    "    print (k, mu, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init predicted values\n",
    "k_pred = -1 * ones(iris.target.size)\n",
    "\n",
    "# evaluate posterior for each point and find maximum\n",
    "for i in range(iris.target.size):\n",
    "    pmax, kmax = -1, None   # initialize to nonsense values\n",
    "    for k in classes:\n",
    "        num, prior, mu, var = param[k]\n",
    "        diff = iris.data[i,:] - mu\n",
    "        d2 = diff*diff / (2*var) \n",
    "        p = prior * np.exp(-d2.sum()) / np.sqrt(np.prod(2*pi*var))\n",
    "        if p > pmax:\n",
    "            pmax = p\n",
    "            kmax = k\n",
    "    k_pred[i] = kmax\n",
    "\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (iris.target.size, (iris.target!=k_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run sklearn's version - read up on differences if interested\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(iris.data, iris.target).predict(iris.data)\n",
    "\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (iris.target.size, (iris.target!=y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same results from both methods?\n",
    "np.alltrue(k_pred==y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# class probabilities\n",
    "gnb.predict_proba(iris.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Still cheating!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros and Cons\n",
    "\n",
    "- Features are automatically treated correctly relative to each other\n",
    "\n",
    "> For example, measuring similar things in different units? 1m vs 1mm\n",
    "><br><br>\n",
    "> The estimated mean and variance puts them on a meaningful scale\n",
    "\n",
    "- Independence is a strong assumption and for no good reason\n",
    "\n",
    "> Actually... it helps with the computational cost!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Gaussian Naive Bayes\n",
    "\n",
    "- Use the provided [training](Class-Train.csv) and [query](Class-Query.csv) sets to perform classification\n",
    "\n",
    "> **Training** set consists of 3 columns of ($x_i$, $y_i$, $C_i$)\n",
    "\n",
    "> **Query** set only has 2 columns of ($x_i$, $y_i$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Covariance Matrix\n",
    "\n",
    "- Estimate the full covariance matrix for the classes\n",
    "\n",
    ">$\\displaystyle {\\cal{}L}_{\\!\\boldsymbol{x}}(C_k) =  G(\\boldsymbol{x};\\mu_k, \\Sigma_k)$\n",
    "><br><br>\n",
    "> Handles correlated features well\n",
    "\n",
    "- Consider binary problem with 2 classes\n",
    "\n",
    "> Taking the negative logarithm of the likelihoods we compare\n",
    "><br><br>\n",
    ">$\\displaystyle (\\boldsymbol{x}\\!-\\!\\boldsymbol{\\mu}_1)^T\\,\\Sigma_1^{-1}(\\boldsymbol{x}\\!-\\!\\boldsymbol{\\mu}_1) + \\ln\\,\\lvert\\Sigma_1\\lvert$ vs.\n",
    "><br><br>\n",
    ">$\\displaystyle (\\boldsymbol{x}\\!-\\!\\boldsymbol{\\mu}_2)^T\\,\\Sigma_2^{-1}(\\boldsymbol{x}\\!-\\!\\boldsymbol{\\mu}_2) + \\ln\\,\\lvert\\Sigma_2\\lvert$\n",
    "><br><br>\n",
    "> If the difference is lower than a threshold, we classify it accordingly\n",
    "\n",
    "- This is called **Quadratic Discriminant Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same Covariance Matrix\n",
    "\n",
    "- When $\\Sigma_1=\\Sigma_2=\\Sigma$, the quadratic terms cancel from the difference\n",
    " \n",
    ">$\\displaystyle (x\\!-\\!\\mu_1)^T\\,\\Sigma^{-1}(x\\!-\\!\\mu_1) $ \n",
    ">$\\displaystyle -\\ (x\\!-\\!\\mu_2)^T\\,\\Sigma^{-1}(x\\!-\\!\\mu_2) $\n",
    "\n",
    "- Hence this is called **Linear Discriminant Analysis**\n",
    "\n",
    "> Fewer parameters to estimate during the learning process\n",
    "><br><br>\n",
    "> Good, if we don't have enough data, for example...\n",
    "><br><br>\n",
    "> Think linear vs quadratic fitting and how you decide between those"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: QDA and LDA\n",
    "\n",
    "- Use the provided [training](Class-Train.csv) and [query](Class-Query.csv) sets to perform classification\n",
    "\n",
    "> **Training** set consists of 3 columns of ($x_i$, $y_i$, $C_i$)\n",
    "><br><br>\n",
    "> **Query** set only has 2 columns of ($x_i$, $y_i$)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unhomework\n",
    "\n",
    "- Visualize the results in the 2D features space\n",
    "- Make these simple codes run faster by getting rid of the `for` loops, etc.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
