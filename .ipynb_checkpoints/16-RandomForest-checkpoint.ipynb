{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<hr/>\n",
    "\n",
    "# Introduction to Data Science\n",
    "**Tamás Budavári** - budavari@jhu.edu <br/>\n",
    "\n",
    "- Decision trees\n",
    "- Cross-validation\n",
    "- Random forests\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <h1><font color=\"darkblue\">Decision Tree</font></h1>\n",
    "\n",
    "- Recursive partitioning of the training dataset\n",
    "\n",
    "> At a given node with dataset $D$ we look for the\n",
    "> best split \n",
    "> <br>\n",
    "> <br>\n",
    "> $\\theta = $ (feature $j$, threshold $t$) \n",
    "> <br>\n",
    "> <br>\n",
    "> such that the \n",
    "> partitions $D_{\\textrm{left}}(\\theta)$ and $D_{\\textrm{right}}(\\theta)$ have minimal *impurity*\n",
    "> <br>\n",
    "> <br>\n",
    "> $\\displaystyle I(\\theta) = \\frac{n_{\\textrm{left}}}{n}\\, H\\Big(D_{\\textrm{left}}(\\theta)\\Big) + \\frac{n_{\\textrm{right}}}{n}\\,H\\Big(D_{\\textrm{right}}(\\theta)\\Big)$\n",
    "\n",
    "- Different impurity functions $H(\\cdot)$\n",
    "\n",
    "> E.g., Gini with $K$ classes in the partition $D$\n",
    "> <br>\n",
    "> <br>\n",
    ">$\\displaystyle H(D) = \\sum_{i=1}^K p_i (1-p_i)$\n",
    "> <br>\n",
    "> <br>\n",
    "> Or variance for regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:,:2] # only first 2 features\n",
    "c = iris.target\n",
    "subset = c<2 # classes 0 and 1\n",
    "X,c = X[subset,:], c[subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subplot(111,aspect='equal'); \n",
    "scatter(X[:,0],X[:,1],c=c,cmap=cm.brg);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "# grid of points within the limits\n",
    "h = 0.01\n",
    "x,y = meshgrid(arange(X[:,0].min()-.5, X[:,0].max()+.5, h),\n",
    "               arange(X[:,1].min()-.5, X[:,1].max()+.5, h))\n",
    "grid = np.c_[x.ravel(),y.ravel()]\n",
    "\n",
    "# decision-surfaces as fn of max depth\n",
    "for depth in range(1,5):\n",
    "    clf = tree.DecisionTreeClassifier(max_depth=depth)\n",
    "    z = clf.fit(X,c).predict(grid).reshape(x.shape)\n",
    "    figure(figsize=(4,4)); subplot(111,aspect='equal')\n",
    "    contourf(x, y, z, cmap=cm.Spectral_r)\n",
    "    scatter(X[:,0], X[:,1], c=c, cmap=cm.gray);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# quality as fn of max depth\n",
    "for depth in range(1,5):\n",
    "    clf = tree.DecisionTreeClassifier(max_depth=depth)\n",
    "    s = cross_val_score(clf, X,c, cv=10) # apples to apples?\n",
    "    print (depth, s.mean(), s.std(), s.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "k_fold = KFold(n_splits=10,shuffle=True,random_state=42) \n",
    "\n",
    "scores = dict() # store the cv score of each split\n",
    "for train, test in k_fold.split(X):\n",
    "    for depth in range(1,5):\n",
    "        clf = tree.DecisionTreeClassifier(max_depth=depth)\n",
    "        clf.fit(X[train],c[train])\n",
    "        score = clf.score(X[test],c[test])\n",
    "        if depth not in scores: scores[depth] = [] # empty \n",
    "        scores[depth].append(score) # list for this depth\n",
    "\n",
    "for depth in scores:\n",
    "    s = np.array(scores[depth])\n",
    "    print (depth, s.mean(), s.std(), s.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font color=\"darkblue\">Random Forest</font></h1>\n",
    "\n",
    "### Random Tree\n",
    "\n",
    "- Hard to pick the *best* in high dimensions (i.e., very expensive)\n",
    "\n",
    "> Instead we pick a number of random directions to search<br/>\n",
    "> and take the optimal split among those\n",
    "\n",
    "- A randomized tree will not be optimal but much faster to build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forest of Random Trees\n",
    "\n",
    "- Create multiple randomized trees to classify\n",
    "\n",
    "> Combine the results, e.g., voting\n",
    "\n",
    "- Check out Boostrap and Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=50,max_depth=2)\n",
    "z = clf.fit(X,c).predict(grid).reshape(x.shape)\n",
    "\n",
    "figure(figsize=(4,4)); subplot(111,aspect='equal')\n",
    "contourf(x,y,z, cmap=cm.Spectral_r)\n",
    "scatter(X[:,0],X[:,1], c=c, cmap=cm.gray);\n",
    "\n",
    "cross_val_score(clf, X,c, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions and Limitations\n",
    "\n",
    "- Axis parallel boundaries might be suboptimal\n",
    "\n",
    "- But there is no need for a distance function\n",
    "\n",
    "- And improved by random forest with many trees\n",
    "\n",
    "also\n",
    "\n",
    "- Improvements in the estimation error by random forest\n",
    "cf. central limit theorem\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "- How frequently do we use a feature to split?\n",
    "\n",
    "> It says something about how useful that feature is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide and Conquer\n",
    "\n",
    "- These methods are applicable to regression problems\n",
    "\n",
    "> Minimize the variance of the left and right partitions\n",
    "> <br>\n",
    "> e.g., piecewise constant\n",
    "\n",
    "- Useful concepts to consider to build complex methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code it together!\n",
    "\n",
    "- What RF classifier gives the best result on the IRIS dataset?\n",
    "\n",
    "> Vary the relevant parameters to find the best one!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i, j, 4*i+j) for i in range(4) for j in range(4-i) if i*j<5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "c = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k_fold = KFold(n_splits=10,shuffle=True,random_state=42) \n",
    "\n",
    "# Create list of classifiers\n",
    "clfs = [((n,d,f,c),RF(max_depth=d, n_estimators=n, max_features=f, criterion=c)) \n",
    "        for c in ['gini','entropy'] for d in [2,3] for n in [50,200] for f in [2,3]]\n",
    "print (len(clfs))\n",
    "\n",
    "# Use k-fold Cv to find the best\n",
    "scores = dict() # store the cv score of each split\n",
    "for train, test in k_fold.split(X):\n",
    "    for param, clf in clfs:\n",
    "        score = clf.fit(X[train,:],c[train]).score(X[test,:],c[test])\n",
    "        if param not in scores: scores[param] = []\n",
    "        scores[param].append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in scores:\n",
    "    print (param, np.mean(scores[param]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automation: Pipeline and GridSearchCV\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html <br/>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unhomework\n",
    "\n",
    "Go to http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html and modify the code to run all classifiers that we learned so far"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
