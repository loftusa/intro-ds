{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr/>\n",
    "\n",
    "# Introduction to Data Science\n",
    "**Tamás Budavári** - budavari@jhu.edu <br/>\n",
    "\n",
    "- Classification exercises\n",
    "- Cross-validation\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><font color=\"darkblue\">Classification</font></h1>\n",
    "\n",
    "- Based on a **training set** of labeled points, assign class labels to unknown vectors in the **query set**.  \n",
    "\n",
    "> **Training set**\n",
    "><br>\n",
    "><br>\n",
    ">$\\qquad \\displaystyle T = \\big\\{ (x_i, C_i) \\big\\}$ \n",
    "><br>\n",
    "><br>\n",
    "> where $x_i\\in \\mathbb{R}^d$ are feature sets and $C_i$ are the known class memberships\n",
    "\n",
    "<nbsp/>\n",
    "\n",
    "> **Query set**\n",
    "><br>\n",
    "><br>\n",
    ">$\\qquad \\displaystyle Q = \\big\\{ x_i \\big\\}$ \n",
    "><br>\n",
    "><br>\n",
    "> where $x_i\\in \\mathbb{R}^d$ consist of the kind of features in $T$\n",
    "\n",
    "- And again, $x_i$ are not real vectors but **feature sets** of a bunch of scalars in general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes with Covariance Matrix\n",
    "\n",
    "- Estimate the full covariance matrix for the classes\n",
    "\n",
    ">$\\displaystyle {\\cal{}L}_{\\!\\boldsymbol{x}}(C_k) =  G(\\boldsymbol{x};\\mu_k, \\Sigma_k)$\n",
    "><br>\n",
    "> Handles correlated features well\n",
    "\n",
    "- Consider binary problem with 2 classes - using Bayes' rule\n",
    "\n",
    ">$ \\displaystyle \\frac{P(C_1|x)}{P(C_2|x)} = \\frac{\\pi_1}{\\pi_2}\\cdot \\frac{{\\cal{}L}_{\\!\\boldsymbol{x}}(C_1)}{{\\cal{}L}_{\\!\\boldsymbol{x}}(C_2)} $\n",
    "\n",
    "> Taking the negative logarithm, we compare\n",
    "><br><br>\n",
    ">$\\displaystyle (x\\!-\\!\\mu_1)^T\\,\\Sigma_1^{-1}(x\\!-\\!\\mu_1) + \\ln\\,\\lvert\\Sigma_1\\lvert $ \n",
    "> <br> vs.\n",
    "><br>\n",
    ">$\\displaystyle (x\\!-\\!\\mu_2)^T\\,\\Sigma_2^{-1}(x\\!-\\!\\mu_2) + \\ln\\,\\lvert\\Sigma_2\\lvert $\n",
    "><br>\n",
    "><br>\n",
    "> If the difference is higher/lower than a threshold (based on the priors), we classify $x$ accordingly\n",
    "\n",
    "- This is called **Quadratic Discriminant Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same Covariance Matrix\n",
    "\n",
    "- When $\\Sigma_1=\\Sigma_2=\\Sigma$, the quadratic terms cancel from the difference\n",
    " \n",
    ">$\\displaystyle (x\\!-\\!\\mu_1)^T\\,\\Sigma^{-1}(x\\!-\\!\\mu_1) $ \n",
    ">$\\displaystyle -\\ (x\\!-\\!\\mu_2)^T\\,\\Sigma^{-1}(x\\!-\\!\\mu_2) $\n",
    "\n",
    "- Hence this is called **Linear Discriminant Analysis**\n",
    "\n",
    "> Fewer parameters to estimate during the learning process\n",
    "> <br>\n",
    "> Good, if we don't have enough data, for example...\n",
    "> <br>\n",
    "> Think linear vs quadratic fitting and how you decide between those"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: QDA \n",
    "\n",
    "- Use the provided [training](Class-Train.csv) and [query](Class-Query.csv) sets to perform classification\n",
    "\n",
    "> **Training** set consists of 3 columns of ($x_i$, $y_i$, $C_i$)\n",
    "> <br>\n",
    "> **Query** set only has 2 columns of ($x_i$, $y_i$)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Best class?\n",
    ">$\\displaystyle \\max_k \\big[\\ P(C_k|x)\\ \\big]$\n",
    ">\n",
    ">$\\displaystyle \\max_k \\big[\\ \\pi_k {\\cal{}L}_x(C_k)\\ \\big]$\n",
    ">\n",
    ">$\\displaystyle \\min_k \\big[ -\\ln\\pi_k - \\ln{\\cal{}L}_x(C_k)\\ \\big]$\n",
    "\n",
    "> #### Multivariate normal\n",
    ">$\\displaystyle {\\cal{}L}_x(C_k) = \\frac{1}{\\sqrt{\\lvert2\\pi\\Sigma_k\\rvert}} \\exp\\left(-\\frac{1}{2} (x\\!-\\!\\mu_k)^T \\Sigma_k^{-1} (x\\!-\\!\\mu_k)\\right)$\n",
    ">\n",
    "> Hence,\n",
    ">\n",
    ">$\\displaystyle \\min_k \\Big[ \\frac{1}{2} (x\\!-\\!\\mu_k)^T \\Sigma_k^{-1} (x\\!-\\!\\mu_k) + \\frac{1}{2}\\ln\\lvert\\Sigma_k\\rvert -\\ln\\pi_k \\ \\Big]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyQDA(object):\n",
    "    \"\"\" Template for a classifier\n",
    "    \"\"\"\n",
    "    def fit(self,X,C):\n",
    "        self.param = dict()\n",
    "        # your code here\n",
    "        klist = np.unique(C)\n",
    "        for k in klist:\n",
    "            members = C==k\n",
    "            prior = members.sum() / C.size  # X.shape[0]\n",
    "            S = X[members, :]\n",
    "            mu = S.mean(axis=0)\n",
    "            Z = (S-mu).T  # centered column vectors\n",
    "            cov = Z @ Z.T / (Z.shape[1] - 1)\n",
    "            self.param[k] = (mu, cov, prior)\n",
    "        return self\n",
    "\n",
    "    def predict(self,Y):\n",
    "        Cpred = -1 * np.ones(Y.shape[0])\n",
    "        # use linalg.det(matrix)\n",
    "        # and linalg.inv(matrix)\n",
    "        for i in range(Cpred.size):\n",
    "            d2min, kbest = 1e99, None\n",
    "            for k in self.param:\n",
    "                mu, cov, prior = self.param[k]\n",
    "                diff = (Y[i, :] - mu).T  # 2D column vector\n",
    "                d2 = diff.T @ np.linalg.inv(cov) @ diff / 2\n",
    "                d2 += np.log(np.linalg.det(cov)) / 2 - np.log(prior)\n",
    "                if d2 < d2min:\n",
    "                    kbest = k\n",
    "                    d2min = d2\n",
    "            Cpred[i] = kbest\n",
    "        return Cpred\n",
    "    \n",
    "cls = MyQDA()\n",
    "cls.fit(X,C)\n",
    "Cpred = cls.predict(Q)\n",
    "np.unique(Cpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n",
      "Number of different estimates: 0\n"
     ]
    }
   ],
   "source": [
    "# reference implementation\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "\n",
    "D = np.loadtxt('files/Class-Train.csv', delimiter=',')\n",
    "Q = np.loadtxt('files/Class-Query.csv', delimiter=',')\n",
    "X, C = D[:,0:2], D[:,2]\n",
    "\n",
    "Cpred = MyQDA().fit(X,C).predict(Q)\n",
    "Cskit =   QDA().fit(X,C).predict(Q)\n",
    "\n",
    "print ('Number of different estimates:', (Cpred!=Cskit).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><font color=\"darkblue\">Cross-Validation</font></h1>\n",
    "\n",
    "- How to evaluate the quality of estimator?\n",
    "\n",
    "> $k$-NN method's parameter affects the results\n",
    "\n",
    "- We saw on the IRIS data that 1-NN was overfitting\n",
    "\n",
    "> We discussed excluding the point itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitions of the Training set\n",
    "\n",
    "- Random complementary subsets \n",
    "\n",
    "> Train on a larger subset, test on a small\n",
    "> <br>\n",
    "> Multiple rounds to decrease variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave-One-Out\n",
    "\n",
    "- For each point, we train on the others and test\n",
    "\n",
    "> Testing on $n$ points requires $n$ trainings\n",
    "\n",
    "- Expensive!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A Relaxed Variant\n",
    "\n",
    "- $k$-fold cross-validation \n",
    "\n",
    "> 1. Create $k$ partitions of equal sizes, e.g., $k=2$ yields two subsets\n",
    "> 2. Pick a single partition and train on the other $(k\\!-\\!1)$ \n",
    "> 3. Repeat for all $k$ partitions - requires $k$ trainings\n",
    "\n",
    "- Leave-One-Out is a special case with $k=n$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Cross-Validation\n",
    "\n",
    "- Evaluate QDA on the [training](files/Class-Train.csv) set using 2-fold cross-validation\n",
    "\n",
    "> 1. What is the fraction of correct estimates? \n",
    "> 2. What is the uncertainty of that fraction?\n",
    " \n",
    "> The **training** set consists of 3 columns of ($x_i$, $y_i$, $C_i$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (156, 3) (157, 3)\n",
      "[0. 1.]\n",
      "1 (157, 3) (156, 3)\n",
      "[0. 1.]\n"
     ]
    }
   ],
   "source": [
    "Dc = D.copy()\n",
    "# randomize and split to D1 + D2\n",
    "np.random.seed(seed=42)\n",
    "np.random.shuffle(Dc)\n",
    "split = Dc.shape[0] // 2  # or int(Dc[:,0].size/2)\n",
    "D1, D2 = Dc[:split,:], Dc[split:,:]\n",
    "\n",
    "# train on one, estimate on the other\n",
    "for i, (T, Q) in enumerate([(D1, D2), (D2, D1)]):\n",
    "    print(i, T.shape, Q.shape)\n",
    "    X, C = T[:, :2], T[:, 2]\n",
    "    CPred = MyQDA().fit(X,C).predict(Q[:, :2])\n",
    "    Ctrue = Q[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done already?\n",
    "\n",
    "- Visualize the results in the 2D features space\n",
    "- Make these simple codes run faster \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unhomework\n",
    "\n",
    "- Implement LDA and compare to sklearn\n",
    "\n",
    ">1. Write code without using sklearn \n",
    ">2. Apply to [training](Class-Train.csv) and [query](Class-Query.csv) sets \n",
    ">3. Compare your results to sklearn's \n",
    "\n",
    "- Perform 10-fold cross-validation of *MyQDA* on [this](Class-Train.csv) file\n",
    "\n",
    ">1. Write code without using `sklearn` \n",
    ">2. Calculate average number of good classifications \n",
    ">3. Compare to sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "clf = QDA()\n",
    "cross_val_score(clf, X,C, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does this mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "k_fold = KFold(n_splits=10, shuffle=False) \n",
    "\n",
    "for k, (train, test) in enumerate(k_fold.split(X)):\n",
    "    clf.fit(X[train],C[train])\n",
    "    Cpred = clf.predict(X[test])\n",
    "    print (k, ':\\t', (C[test]==Cpred).sum() / float(test.size),\n",
    "        '  =  ', clf.score(X[test],C[test]) )"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
